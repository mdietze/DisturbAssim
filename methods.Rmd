---
output:
  pdf_document: default
  html_document: default
---
# Methods

## VSEM (Very Simple Ecosystem Model)

To illustrate the problem of assimilating disturbance, and present potential solutions, we make use of the Very Simple Ecosystem Model (VSEM), a daily-timestep point-scale model developed by David Cameron that is available in the BayesianTools R package [@BayesianTools]. VSEM contains three carbon pools, leaf (Cl), wood (Cw), and soil (Cs), and calculates leaf area index based on specific leaf area (SLA), $LAI = SLA*Cl$. Gross Primary Productivity (GPP) is estimated based on incoming photosynthetically active radiation (PAR), light use efficiency (LUE), and Beer's Law light interception ($1-exp(-K_{ext}\cdot LAI)$) with a extinction coefficient $K_{ext}$. Autotrophic respiration, Ra, is a fixed fraction, $\Gamma$ of GPP. Of the remaining carbon (net primary productivity), a fixed fraction, $Av$, is allocated to leaves and the remainder is allocated to wood. Leaves and wood turnover into soil C based on fixed longevities, $\tau_l$ and $\tau_w$, while heterotrophic respiration, Rh, occurs based on a fixed residence time $\tau_s$. Net Ecosystem Exchange follows the atmospheric sign convention $Ra + Rh - GPP$. The VSEM was selected for its speed and ease of understanding, but its fundamental structure is analogous to much more complex, large-scale terrestrial carbon models. 

## Ensemble Kalman Filter

The Ensemble Kalman Filter (EnKF) [@Evensen2009; @Evensen2009a] is a popular data assimilation algorithm that we use both to demonstrate the challenges conventional data assimilation approached have with capturing discrete disturbances and as our jumping off point for developing more flexible methods. Like most data assimilation algorithms the EnKF involves two cyclic steps: a Forecast step where a model is used to probabalistically predict the system state (and any other outputs) forward in time, and an Analysis step where new observations are used to update system state variables (e.g. carbon pools) and sometimes model parameters. These updated variables are then used as the initial conditions for the next Forecast round.

Within the EnKF, the probabalistic Forecast step is performed using an ensemble of model runs that vary in at least initial conditions, but potentially also model parameters, drivers, and process error. The ensemble forecast approach is easy to apply to any model, is particularly good at capturing nonlinear predictions, and unlike many other approaches does not require solving for the adjoint of the model. In EnKF the forecast, $f$, at any point in time, $t$, is then summarized in terms of a vector of means across ensemble members, $\mu_{f,t}$ and the sample covariance, $P_{f,t}$. In our example we are interested in the three state variables in the VSEM, ${C_l,C_w,C_s}$, at a single site and assume an ensemble size of 200. Therefore $\mu_{f,t}$ is a vector of length 3 and $P_{f,t}$ is a $3 \times 3$ covariance matrix. If we were making a spatial forecast at $n_s$ locations, $\mu_{f,t}$ would be a vector of length $3 n_s$ and $P_{f,t}$ would be a $3 n_s \times 3 n_s$ covariance matrix that would capture the full set of correlations across both variables and space. An important difference between data assimilation and spatial statistics is that in data assimilation the spatial covariance matrix comes from (1) the mechanistic understanding embedded in our process model and (2) the spatial covariance in the forecast inputs (initial conditions, parameters, drivers, process error), rather than an empirical semivariogram.

The Analysis step in the EnKF is the same as that in the standard Kalman Filter. Because the forecast step from t-1 to t occurs prior to the observed data at time t, $Y_t$, the forecast of our three state variables is used as an informative _prior_ distribution and is assumed to follow a multivariate Normal (MVN) distribution with mean $\mu_{f,t}$ and covariance $P_{f,t}$. The EnKF also acknowledges that the observed data at time t, $Y_t$, is measured imperfectly. Observation error is assumed to follow a MVN distribution with an observation error covariance matrix $R$. Furthermore, it is assumed that $R$ is known, which is not unreasonable as many derived data products and sensors report uncertainties, and uncertainties from field data can often be inferred from sampling theory or Monte Carlo resampling / bootstrapping. In many cases new data do not exist for every state variable (e.g. we may observe leaves and wood but not soil C) and so an observation operator matrix, $H$ is also introduced, which in most cases is just a $n \times m$ indicator matrix matching the observed $Y$'s (n=2 in this example) with the vector of state variables (m=3 in this example). Indeed, one of the powerful features of data assimilation is its ability to update unobserved state variables based on the covariances between observed and unobserved variables. Putting this all together, Bayes' Theorem can then be used to update our MVN forecast prior, given a MVN Likelihood of $Y_t$ with covariance $R$, giving an analytically-tractable conjugate MVN posterior with mean $\mu_{a,t}$ and covariance $P_{a,t}$:

$$P_{a,t}^{-1} = H_t^T R_t^{-1} H_t + P_{f,t}^{-1}$$
$$\mu_{a,t} =  P_{a,t}^{-1}\left( H_t^T R^{-1} Y_{t} + P_{f,t}^{-1} \mu_{f,t} \right) $$

The subscript $a$ is used to refer to the Analysis posterior and distinguish this from the prior forecast $f$ mean and covariance for the same timestep. While the matrix notation in these equations can make these solutions look complicated, the underlying meaning is accessible. First, the precision of the analysis (i.e. 1/variance) is the sum of the observation precision and the forecast precision, meaning that the analysis is always more precise than the data or forecast alone. Second, the mean of the analysis is a weighted average between the data, $Y_t$ and the forecast $mu_{f,t}$, each weighted by their precision. This averaging can also be viewed as a 'nudge', pushing the forecast toward the new data, with the magnitude of that nudge determined by the uncertainties in both the forecast and data (i.e. highly precise data = big nudge, highly uncertain data = very small nudge).

While this fusion of forecast and new data is an incredibly powerful concept, as alluded to in the _Introduction_ there are times that the Kalman Normal-Normal "nudge" produces results that don't always make sense. Consider as a simple example a simulated data experiment where LAI growth follows a simple stochastic logistic model with growth rate $r$=0.1, equilibrium $K$=6, process error standard deviation 0.25, and observation error standard deviation 0.75. Under normal conditions, EnKF would have no trouble nudging a LAI forecast back on track every time a new observation is made, but what if a discrete disturbance event occurs knocking LAI from steady-state down to $1.0 \pm 0.5$? In this case the compromise between the forecast ($LAI=6$) and observations ($LAI=1$) produces a compromise ($LAI \approx 5$ that is a poor representation of the state of the system (Figure \@ref(fig:enkf)). Furthermore, as the true LAI slowly recovers upward, the EnKF keeps gradually nudging LAI downward until the two converge. Not only is the assimilated state wrong, but so is our estimate of the process -- a discrete disturbance and rapid recovery is behaving like gradual degradation, which will lead to incorrect inferences about related processes (litter, soil C, NPP, NEE, ET, albedo, etc.). Not also that including a disturbance probability in the process model does not solve this problem. Assuming the disturbance probability is low (e.g. a background rates of <1%), at every point in time the forecast would include a small number of disturbed ensemble members, but the overwhelming majority would be undisturbed. Therefore $\mu_{f,t}$ will be relatively unchanged and $P_{f,t}$ would see a small increase in variance. The fundamental problem is that even though the true forecast is now bimodal (a large peak at the undisturbed state and a much smaller peak at the disturbed state), the EnKF is still representing this as a unimodal Normal. Indeed, if the you increased the disturbance probability substantially (e.g. 50%), the EnKF would treat this as a unimodal forecast half way between the disturbed and undisturbed states, a place where no ensemble members are present (Figure \@ref(fig:concept), top).

```{r enkf, echo=FALSE, fig.cap="Example assimilation of a disturbance event shows that conventional assimilation approaches, such as EnKF, take time to converge to new states after disturbance (orange). By modeling the probability of disturbance (black) our Multinomial assimilation (blue) can correctly jump between states during disturbance events (e.g. P(disturbance) = 0.977 at time 10) while remaining resilient to potential false-positive events (e.g. time 12 and 15)."}
SSSD_figure <- function(){
  load("SSSD.RData")      ## simulation from SingleSite_SingleDisturbance.Rmd
  plot(data$Y,ylim = range(rbind(CI.fN,data$Y)),xlim=xlim,type='n',
     ylab="LAI",xlab="time")
  polygon(cbind(c(time, rev(time), 1), c(an0.stats[2, ], rev(an0.stats[4, ]),
    an0.stats[1, 1])), border = NA, col = '#FFBF6C')   ## EnKF interval
  ecoforecastR::ciEnvelope(1:NT,CI.fN[2,],CI.fN[4,],col=rgb(col[1],col[2],col[3],0.3*256,maxColorValue=256)) ## Multinomial interval
  lines(an0.stats[3,],col="orange",lwd=2)   ## EnKF median
  lines(CI.fN[3,],col="blue",lwd=2)         ## Multinomial median
  lines(Dbar,lwd=2)                         ## Multinomial disturbance rate
  points(data$Y,pch=18,col=2)               ## data
  legend("topright",legend=c("EnKF","Multinomial","data","P(dist)"),
       lwd=5,col=c("orange","blue","red","black")) ##"#b2df8a"
}
SSSD_figure()
```

```{r concept, echo=FALSE, message = FALSE, fig.cap="Top: In a conventional EnKF, a multimodal forecast (grey histogram) is approximated by a unimodal Normal (green), such that when a low probability disturbance is actually observed (red) the posterior (orange) does not capture this event. Bottom: In the multinomial filter the same multimodal forecast is represented by a mixture of normals (green and dark red) each weighted by their prior (forecast) probabulities. The posterior (blue), updates the mean and variance of each mode as well as their relative probabilities, in this case capturing the high probability that a disturbance occurred."}
library(rjags)
par(mfrow=c(2,1))
## params
set.seed(1066)
p.dist = 0.05
n0 = 1
nIC = 6
r = .1
K = 6
n.sd = 0.25
NE = 1000
obs=1
obs.sd = 0.75


## generate IC ensemble
ens = rnorm(NE,nIC,n.sd)
  
## Forecast step
LAI = ens + r*ens*(1-ens/K) + rnorm(NE,0,n.sd) 
disturbed = as.logical(rbinom(NE,1,p.dist))
LAI[disturbed] = abs(rnorm(sum(disturbed),n0,0.5)) ## disturbance

##Approximate forecast with Normal
xseq = seq(0,8,length=1000)
muf = mean(LAI)
Pf = var(LAI)

## analysis step: combine previous forecast with observed data
H <- matrix(1,ncol=1)
K <- Pf * t(H) %*% solve(H%*%Pf%*%t(H) + obs.sd^2)
mua <- muf + K*(obs - muf) 
Pa <- (1-K %*% H)*Pf    

## EnKF
hist(LAI, probability = TRUE, main = "Conventional EnKF")#,ylim=c(0,dnorm(mua,mua,sqrt(Pa))))
lines(xseq,dnorm(xseq,muf,sqrt(Pf)),col="#b2df8a",lwd=3)
lines(xseq,dnorm(xseq,obs,obs.sd),col='red',lwd=3)
lines(xseq,dnorm(xseq,mua,sqrt(Pa)),col='orange',lwd=3)
legend("topleft",legend=c("forecast","data","assimilation"),lwd=3,col=c("#b2df8a","red","orange"))

## Forecast: Mixture of normals
mu1 = mean(LAI[!disturbed])
P1 = var(LAI[!disturbed])
mu2 = mean(LAI[disturbed])
P2 = var(LAI[disturbed])

## assimilation
AnalysisSSSD <- "
model{
  ## prior
  muN ~ dnorm(mufN,pfN)
  muD ~ dnorm(mufD,pfD)
  D ~ dbern(p)
  ## Analysis
  n <- D*muD + (1-D)*muN
  Y ~ dnorm(n,r)
}"
priors <- list(p = p.dist,
                 mufN = mu1,
                 mufD = mu2,
                 pfN  = 1/P1,
                 pfD  = 1/P2,
                 Y = obs,
                 r = 1/obs.sd^2
                 )
mod <- jags.model(file=textConnection(AnalysisSSSD),
                   data=priors,n.adapt=1000,n.chains=3)
jdat <- coda.samples(mod,variable.names=c("n"),n.iter=10000) 

hist(LAI,probability = TRUE,ylim=c(0,dnorm(mu1,mu1,sqrt(P1))), main="Multinomial Filter")
lines(xseq,(1-p.dist)*dnorm(xseq,mu1,sqrt(P1)),col="#b2df8a",lwd=3)
lines(xseq,p.dist*dnorm(xseq,mu2,sqrt(P2)),col="brown",lwd=3)
lines(xseq,dnorm(xseq,obs,obs.sd),col='red',lwd=3)
lines(density(as.matrix(jdat)),col="blue",lwd=3)
legend("top",legend=c("undisturbed","disturbed","data","assimilation"),lwd=3,col=c("#b2df8a","brown","red","blue"))
```

## Derivation of the Multinomial Filter

To address this fundamental flaw in the Kalman Analysis, we need to build into the Analysis step both the possibility for ensemble forecasts to be multimodel, and the ability for the data assimilation to jump discretely between modes all-at-once rather than through gradual nudges. To begin, let's use $X$ to represent the latent (unobserved) "true" state of the system. Furthermore, we will consider a process-model that is capable of predicting $k$ different categorical outcomes (land cover classes, types of disturbance, etc.) each of which has its own prediction for the latent continuous state variables of interest, which we'll refer to as $X^L_{k,t}$. Similar to the EnKF we'll assume that the ensemble prediction for any one of these categories can be represented as a multivariate Normal based on the ensemble sample mean and covariance:

$$X^L_{k,t} \sim MVN(\vec{\mu}_{f,k,t},P_{f,k,t})$$
  
However, because we have $k$ categories we end up with a multimodel prediction. Furthermore, these modes need not be distinct; it is perfectly fine in concept for these MVNs to be overlapping. In practice they more they overlap the harder it is to identify which mode you are in, but as we will see this categorical assignment will be treated probabalistically. Indeed, to be able to represent our multimodel prediction as a single probability distribution we need to know the _forecasted_ probability of being in each category, which we'll take as known and represent using the vector $\vec{p_t}$, giving us a weighted MVN mixture as our forecast.

In practice, an individual location can only be in one of these $k$ states, which we'll model using a Multinomial distribution with a sample size of 1.
  
$$\vec{\rho_t} \sim Multinomial(1,\vec{p_t})$$ 
  
This is a generalization of the Bernoulli distribution (Binomial with n=1) and for any individual draw from the Multinomial $\vec{\rho_t}$ will be a vector of length $k$ with a 1 in the category being assigned and a 0 for all other categories. 

Within the analysis step we will retain the assumption that the observed data $Y_t$ follow a multivariate Normal distribution around the true latent $X_t$ with observation error $R_t$

$$Y_t \sim MVN(X_t,R_t)$$

We then model $X_t$ as the product of the matrix $X^L_t$ and the vector $\vec{\rho_t}$

$$X_t = X^L_t \vec{\rho_t}$$
which for any individual draw of $\vec{\rho_t}$ assigns $X_t$ that specific $X^L_{k,t}$. In practice, we will use Bayes' Theorem to integrate over the uncertainties in both our categorical assignment and continuous state variables, producing posterior Analysis probability distributions for both $X_t$ and $\vec{\rho_t}$. Unlike in the EnKF, where conjugacy allows us to calculate an analytical solution for the posterior, there is not simple analytical solution for the joint posterior distribution of $X_t$ and $\vec{\rho_t}$. In this case we will use well-developed Bayesian Markov Chain Monte Carlo (MCMC) numerical algorithms to sample from the posterior distribution. Specifically, numerical analyses in this paper were conducted using JAGS v4.2.0 [@Plummer03] accessed from R v4.0.3 [@Rcore] using the rjags package v4.10 [@rjags].

### Ensemble Adjustment

When making ensemble forecasts using the Analysis from the traditional EnKF, one needs to sample new initial conditions from the Analysis joint posterior distribution. This work fine if the initial conditions are complete (100% of the model variables represented) and when there is no covariance with any of the other uncertainties being propagated (e.g. sampling ensemble drivers or posterior parameter distributions). In practice, this assumption is frequently violated, for example an above average ensemble member might be above average not just because of its initial condition, but also because it had favorable parameters or drivers, creating a complex covariance between states, parameters, and drivers that would be destroyed by randomly resampling initial conditions. Because of this Anderson et al. [-@Anderson2001] developed an alternative know as _ensemble adjustment_ that nudges individual ensemble members toward the posterior analysis mean and covariance. Mathematically, the approach is essentially the multivariate generalization of using the prior forecast mean and variance to calculate a Z-score for every ensemble member (i.e. express as an standard Normal anomaly with mean 0 and standard deviation 1), and then apply that Z-score to the new posterior analysis mean and variance.

In our Multinomial filter it is important to generalize this concept of ensemble adjustment to both discretely move ensemble members from one category to another, and then to preserve the covariances among state variables, and with drivers and parameters, when updating the discrete categorical assignments of individual ensemble members. To reassign forecasted ensemble members to new disturbance classes we first identify the "sink" categories that increased in probability between the prior, $p_{k,t}$, and the posterior, $\bar{\rho}_{k,t}$.

$$\delta p_{k,t} = max\{0,\bar{\rho}_{k,t}-p_{k,t} \}$$

For "source" ensemble members that belong to categories that decreased in relative abundance, the excess is reassigned to new classes according to a Multinomial draw.

$$c_{A|k} = Multinomial\left( n_k, \frac{\vec{\delta p_t}}{\sum \vec{\delta p_t}}  \right)$$ 

Conditional on this reassignment, the continuous states are updated according to [@Anderson2001] using the posterior statistics for that $X^L_{k,t}$. It is also worth noting that some cases of class reassignment may also require keeping track of categorical switches in Plant Functional Type (PFT), such as a transition from forest to agriculture, which implies a large jump in PFT parameters as well. One could apply the same [@Anderson2001] style adjustment to PFT parameters, but for simplicity here we just maintained alternative parameter ensembles for each PFT, with each ensemble member constructed with the identical anomalies around each PFT's mean..

## Simulated data experiments

To test our algorithm we applied it to a series of increasingly complex simulated data experiments where we know the "true" state of the system, including the timing and category of disturbance events. The first of these is the logistic LAI model described previously in the EnKF section (single pool, single site, single disturbance. 

```{r pseudoTable,echo=FALSE}
## table summarizing simulated data experiments
exps = data.frame(exp=1:5,
                  pools= c(1,3,3,3,3),
                  disturbances = c(1,1,2,2,1),
                  PFTs = c(1,1,1,2,1),
                  sites = c(1,1,1,1,2)
                  )
knitr::kable(exps,caption="Simulated data experiments")
```


For the second experiment, we used the VSEM to extend the Multinomial assimilation model to a single site, single disturbance example with multiple pools. In this and the following simulated data experiments we used VSEM's default parameters, drawing ensemble members from Normal distributions centered on the prior mean and assuming a standard deviation that was 1/10th the default uniform prior range. The goal here was not to replicate any specific system, but to provide an amount of parameter uncertainty that was realistic but not so large as to generate trivial ensemble forecasts. Model runs were for 25 years using VSEM's default simulated PAR as the driver and with data assimilation occurring annually. Model initial conditions were generated by an initial 25 year ensemble spin-up prior to the 25 year experiment. Process error was added to the simulation annually assuming a zero-bound tobit distribution with a process error covariance matrix that was diagonal with variances of 0.1 for all pools. Ensemble members experienced disturbance stochastically with a 5% annual probability. For the disturbance itself, the post-disturbance leaf and wood carbon were drawn from a tobit with a mean of 1 [UNITS?] for both pools, standard deviations of 0.15 and 0.25 respectively, and 25% of the biomass lost was assumed to enter the soil C pool, with the remainder removed (which could be thought of as either atmospheric emissions or biomass harvest depending on the disturbance type). 

Pseudodata for assimilation was generated by selecting the parameters and initial conditions from one ensemble member at random and running a forward simulation with an imposed disturbance event in year 10. Observation error was then randomly added to the model's predictions using a tobit distribution with the same known observation error matrix, R, as used in the assimilation, which was a diagonal covariance matrix with variances of 0.25, 10, and 1 for leaf, soil, and wood respectively. Within the assimilation the prior on the disturbance rate was 5%.

Our third experiment was similar to the second but introduced a second disturbance type (single-site, multi-disturbance, multi-pool), both with a 5% probability. For both disturbances leaf biomass was reduced to 10% of their pre-disturbance values with a standard deviation of 0.15; in the first disturbance type wood biomass was also reduced to 10% with a standard deviation of 0.25, similar to a stand-clearing disturbance, while in the second 95% of the original wood biomass was retained, with a standard deviation of 0.01, similar to a defoliation event. The prescribed disturbance in the pseudodata was the first type. In this scenario, the Multinomial assimilation needs to leverage the wood data to distinguish between disturbance types because the leaf biomass reduction would be identical.

Our forth experiment was similar to the third, but extended the single-site, multi-disturbance, multi-pool scenario to one where one disturbance type included a switch in PFT. Specifically PFT 2 was identical to the VSEM default, but PFT 1 was parameterized with a stem residence time that was increased by 10x to create a more tree-like PFT. In addition, we established a transition matrix where disturbance 1 (stand clearing) transitions from PFT1 to PFT2, while disturbance 2 (defoliation) keeps the current PFT is. As with the previous scenario, the prescribed disturbance in the pseudodata was type 1 (stand clearing, transition from forest to shrub).

Our fifth and final experiment extended the analysis to a two-site scenario with spatial covariance across sites and pools, but with a disturbance occurring in one site but not at the other. To keep things simpler, this experiment reverted to a single disturbance type, same as experiment 2, except we dropped the disturbance probability to 2.5%. This scenario was designed to test a concern we had in previous research that a disturbance occurring at one site could cause the biomass to decline at other undisturbed sites because of the cross-site covariances [CITE Viskari, Hamze].
     
## Remotely-sensed disturbance in Oregon

Having tested the Multinomial assimilation against simulated data we next applied the algorithm to multiple remotely-sensed disturbance types for an evergreen forested region in the central Oregon Cascades. In summary, we: (1) calibrated VSEM against field data, then (2) used a remotely-sensed disturbance product to construct priors for disturbance rates and PFT transition, then (3) extracted and assimilated ~400 time-series of remotely sensed aboveground biomass, stratified by different disturbance types. We then assessed the ability of the algorithm to detect different disturbances types and magnitudes. 

### Study region and data sources

Our study region was a Landsat tile over central Oregon, USA located between 120 47'W and 123 7'W longitude and 43 45'N and 45 '4'N latitude. For this tile we extracted data from three derived Landtrendr datasets (http://emapr.ceoas.oregonstate.edu/pages/data/viz/index.html, tile h03v04):  biomass, cover, and disturbance. All three products were 30m resolution and annual timestep;biomass and cover extended from 1990-2017, while disturbance extended from 1984-2011 and a nearest neighbor algorithm was used to regrid the disturbance data to be the same as biomass and cover. The biomass product calibrated Landtrendr-processed Landsat data against field plot data using the gradient nearest neighbor (GNN) methodology to produce an annual biomass estimates [@Kennedy2018]. The cover product trained a spatiotemporal exploratory model (STEM), which uses an ensemble of regression trees defined within spatially overlapping support sets, against the 2001 U.S. National Land Cover Database (NLCD) to produce annual land-cover maps for Washington, Oregon, and California [@HOOPER2018]. To similify the analysis we focused on a single vegetation type, evergreen forest, which is the dominant vegetation cover in the region and consists primarily of Ponderosa pine (_Pinus ponderosa_). The disturbance product **need textfrom Robert** [@KENNEDY2010]. To simplify analyses we aggregated the disturbance product into five classes: undisturbed, cut, fire, pest, and other.

```{r map, echo=FALSE, fig.cap="Remotely sensed observations from central Oregon for 2004 used as inputs to test the multinomial filter against real-world disturbances"}
## maps of pre-disturbance biomass, cover, and 2004 disturbance

m = matrix(1:4,nrow=2,byrow = TRUE)
layout(m)

## Cover
if(!exists("cov04")) cov04 = raster::raster("cov04.tif")
load("rat.RData")
raster::plot(cov04,col=rat$color,asp=1,main="NLCD Land Cover",
            axis.args=list(at=seq_along(rat$description),labels=rat$description,cex.axis=0.4))
#cov04 = terra::rast("cov04.tif")
#terra::plot(cov04,col=rat$color,asp=1,main="NLCD Land Cover")

## Disturbance
if(FALSE){ 
  ## both these options seem to interpolate/regrid during the projection, turning integer classes into continuous values that don't map the same
  dis04 = terra::rast("dis04.tif")
  dis04ll <- terra::project(dis04,crs(cov04))
  
  dis04 = raster::raster("dis04.tif")
  dis04ll = raster::projectRaster(dis04,crs=crs(cov04))
  #terra::plot(dis04ll,col=c("#ebebeb",'#b04c00','#1f78b4','#faabaa','#b2df8a'),asp=1,main="Landtrendr Disturbance",plg=list(legend=dname)) #"#a6cee3" '#33a02c'
}
if(!exists("dis04"))  dis04 = raster::raster("dis04.tif")
dname = c("Intact","Other","Cut","Fire","Pest")
raster::plot(dis04,col=c("#ebebeb",'#b04c00','#1f78b4','#faabaa','#b2df8a'),asp=1,
            axis.args=list(at=seq(-5,50,length=5),labels=dname,cex.axis=0.4), 
            main="Landtrendr Disturbance") #"#a6cee3" '#33a02c'


## Biomass
if(!exists("bio04")) bio04 = raster::raster("bio04.tif")
raster::plot(bio04,col=c('#ffffff','#edf8fb','#ccece6','#99d8c9','#66c2a4','#2ca25f','#006d2c'),asp=1,main="Landtrendr Biomass")

```

### Construction of disturbance frequency and transition matrix priors

Our Multinomial disturbance assimilation model takes as priors the disturbance probability and the post-disturbance pool sizes (mean and precision). The algorithm also allows the user to specify a transition matrix for pre- and post-disturbance changes in vegetation type. For some ecosystem models, these quantities may be generated dynamically by the model itself as part of the ensemble forecast. Because VSEM does not internally predict disturbances we instead generated these priors using the Landtrendr disturbance product. The disturbance probability prior was calculated simply as the frequency of each disturbance type over the whole time period and the subset of spatial domain that was classified as evergreen forest. The post-disturbance pool size and transition matrix priors were similarly estimated using the remotely sensed biomass and cover estimates (respectively), this time focusing on a sample of 100 disturbance transitions per year for the years 1990-2003 because of the greater computational cost of this estimation process. Because of the potential lags involved in when a disturbance occurred within a year, we considered an evergreen forest disturbance to be a disturbed pixel that was evergreen forest in the year of disturbance or the prior year. Similarly, we considered the pixel to transition out of forest if the cover type in the year of disturbance or the year following disturbance to be non-forest, but filtered out disturbances where that transition only lasted one year before returning to forest (i.e. these were considered forest-to-forest transitions in the matrix, as were pixels that stayed forest the whole time). Similarly, in order to assess the change in biomass associated with disturbance we calculated the maximum biomass in the three years prior to disturbance and the minimum biomass in either the year of the disturbance or the two years after. For all disturbance types we calculated the mean and standard deviation of the change in biomass both in absolute terms and as a proportional change.

### Calibration of VSEM

VSEM was calibrated primarily using data from 2002-2019 from the Ameriflux site US-ME2 (lat.lon), a mature Ponderosa pine stand. VSEM pools (leaf, wood, and soil C) were initialized using the earliest available site-specific data from the Ameriflux BADM (Biomass, Ancillary, Disturbance, Management) dataset. An additional ten years of leaf and wood biomass data were also available from the BADM, which we included as part of the calibration dataset. Also included was the cumulative annual tower NEE (2002-2019). Because we were working with annual data we assumed Normal likelihoods for all three response variables, and because our priority was the correct prediction of aboveground biomass we upweighted the wood likelihood by 4 and downweighted the NEE likelihood by 1/4. 

Prior to calibration we set the leaf area ratio (LAR) to 15.7 m2/kgC and leaf residence time (tauL) to 4.7 years based on data from the BADM and left the light extinction coefficient (KEXT) at its default (0.5). The remaining fit parameters were the light use efficiency (LUE), the fraction of GPP allocated to autotrophic respiration (GAMMA), the fraction of NPP allocated to leaves (Av), the wood and soil C residence times (tauW, tauS), and the residual standard deviations for each of our data constraints (sd_NEE, sd_leaf, sd_wood). The fit was done taking a Maximum Likelihood approach using base R's `optim` function. Finally, because the process error within a dynamical data assimilation system is not identical to the calibration residual error, we estimated the leaf and wood process errors (Ql, Qw) based on the difference between the predicted and observed net annual changes in each of these pools. Because there was not an equivalent annually-observed soil carbon dataset, the soil carbon process error (Qs) was estimated based on the predicted mean annual rate of change under the simplifying assumption that the true soil carbon is in steady-state.

### Assimilation of remotely-sensed data

To assess the performance of the Multinomial assimilation algorithm against real-world data, we sampled ~100 evergreen forest points for each disturbance class. We specifically sampled disturbance locations from the year 2004, which is sufficiently deep in the past that there is an observable disturbance recovery trajectory, but with a sufficiently long time-series prior to disturbance for the data assimilation to come into steady-state with the biomass data product. For each selected location we extracted a time-series from the remotely-sensed biomass product for use as the data constraint within the assimilation algorithm. The cover and disturbance products were not assimilated, but were just used to construct priors (see above) and assess performance.

For each location we next extracted PAR driver data from the Daymet product [CITE] using the daymetr R package [@daymetr]. For each location the carbon pools were initialized using a multi-step process to account for the lack of leaf and soil C data at a landscape scale. To start we used the first time point in each biomass time-series as the initial condition for the wood pool. Next, using the US-ME2 initial conditions we performed a >10k year spin-up, and then sampled carbon pool ensemble members from the second half of this time-series. Next, for each ensemble member we imposed at disturbance that reduced wood and leaf biomass to 1% of their equilibrum values, then simulated a 200-year recovery trajectory. Finally, for each ensemble member we selected the stand age that minimized the squared error between the modeled biomass and the initial biomass, and then selected the leaf and soil C pools at that stand age. This results is an ensemble of leaf, wood, and soil carbon pools with a covariance structure that is consistent with the calibrated model's dynamics.

For the assimilation experiment, each site was assimilated separately. In the initial round of experiments the observation error variance was set to $26.5\: kg^2/m^2$ based on [@Kennedy2018] Figure 2. Post-assimilation, we then extracted a number of metrics to assess algorithm performance: the posterior probability of disturbance in 2004, the posterior biomass pool size before disturbance, after disturbance, and at the end of the time-series (2017). The difference between the predicted and observed biomass at the end of the assimilation was taken as a metric of filter divergence, the situation where the model in a data assimilation algorithm becomes so confident in itself that assimilated observations have little impact on the posterior, which then diverges from observations. **ONCE RESULTS ARE FINALIZED, COME BACK AND FILL IN WHICH SPECIFIC ANALYSES WERE USED**. 

Finally, based on preliminary results we performed a series of additional assimilation experiments where we manually altered the observation error, R, or process error, Q, to assess the sensitivity of our results. Table (\@ref(tab:QRexp)) lists the multipliers that were used to adjust R and Q. The skill of these assimilation experiments were assessed the same as the original. In addition, changes in skill across experiments were assessed as a function of R and Q.

```{r QRexp,echo=FALSE}
## summary table of experiments
load("QRexp.RData")
QRexp = data.frame(experiment=1:6,Qmult=Qmult[Qs],Rmult=Rmult[Rs])
knitr::kable(QRexp,caption="Data Assimilation experiments")
```

  * Score: filter divergence, disturbance detection & skill
  * Regress vs: pre-disturbance biomass, size of disturbance (abs & rel), type, etc
  
Divergence

As a metric to assess the prevalence of filter divergence, we calculated the difference between the last observed data point and the posterior mean of the corresponding Analysis, and then divided this by half the width of the Analysis 95% confidence interval. If the confidence interval is symmetric, then any Analysis mean that falls in the confidence interval will score between -1 and 1, while values outside this range are suggestive of filter divergence.


    
    

