# Methods

## VSEM (Very Simple Ecosystem Model)

To illustrate the problem of assimilating disturbance, and present potential solutions, we make use of the Very Simple Ecosystem Model (VSEM), a daily-timestep point-scale model developed by David Cameron that is available in the BayesianTools R package [@BayesianTools]. VSEM contains three carbon pools, leaf (Cl), wood (Cw), and soil (Cs), and calculates leaf area index based on specific leaf area (SLA), $LAI = SLA*Cl$. Gross Primary Productivity (GPP) is estimated based on incoming photosynthetically active radiation (PAR), light use efficiency (LUE), and Beer's Law light interception ($1-exp(-K_{ext}\cdot LAI)$) with a extinction coefficient $K_{ext}$. Autotrophic respiration, Ra, is a fixed fraction, $\Gamma$ of GPP. Of the remaining carbon (net primary productivity), a fixed fraction, $Av$, is allocated to leaves and the remainder is allocated to wood. Leaves and wood turnover into soil C based on fixed longevities, $\tau_l$ and $\tau_w$, while heterotrophic respiration, Rh, occurs based on a fixed residence time $\tau_s$. Net Ecosystem Exchange follows the atmospheric sign convention $Ra + Rh - GPP$. The VSEM was selected for its speed and ease of understanding, but its fundamental structure is analogous to much more complex, large-scale terrestrial carbon models. 

## Ensemble Kalman Filter

The Ensemble Kalman Filter (EnKF) [@Evensen2009; @Evensen2009a] is a popular data assimilation algorithm that we use both to demonstrate the challenges conventional data assimilation approached have with capturing discrete disturbances and as our jumping off point for developing more flexible methods. Like most data assimilation algorithms the EnKF involves two cyclic steps: a Forecast step where a model is used to probabalistically predict the system state (and any other outputs) forward in time, and an Analysis step where new observations are used to update system state variables (e.g. carbon pools) and sometimes model parameters. These updated variables are then used as the initial conditions for the next Forecast round.

Within the EnKF, the probabalistic Forecast step is performed using an ensemble of model runs that vary in at least initial conditions, but potentially also model parameters, drivers, and process error. The ensemble forecast approach is easy to apply to any model, is particularly good at capturing nonlinear predictions, and unlike many other approaches does not require solving for the adjoint of the model. In EnKF the forecast, $f$, at any point in time, $t$, is then summarized in terms of a vector of means across ensemble members, $\mu_{f,t}$ and the sample covariance, $P_{f,t}$. In our example we are interested in the three state variables in the VSEM, ${C_l,C_w,C_s}$, at a single site and assume an ensemble size of 200. Therefore $\mu_{f,t}$ is a vector of length 3 and $P_{f,t}$ is a $3 \times 3$ covariance matrix. If we were making a spatial forecast at $n_s$ locations, $\mu_{f,t}$ would be a vector of length $3 n_s$ and $P_{f,t}$ would be a $3 n_s \times 3 n_s$ covariance matrix that would capture the full set of correlations across both variables and space. An important difference between data assimilation and spatial statistics is that in data assimilation the spatial covariance matrix comes from (1) the mechanistic understanding embedded in our process model and (2) the spatial covariance in the forecast inputs (initial conditions, parameters, drivers, process error), rather than an empirical semivariogram.

The Analysis step in the EnKF is the same as that in the standard Kalman Filter. Because the forecast step from t-1 to t occurs prior to the observed data at time t, $Y_t$, the forecast of our three state variables is used as an informative _prior_ distribution and is assumed to follow a multivariate Normal (MVN) distribution with mean $\mu_{f,t}$ and covariance $P_{f,t}$. The EnKF also acknowledges that the observed data at time t, $Y_t$, is measured imperfectly. Observation error is assumed to follow a MVN distribution with an observation error covariance matrix $R$. Furthermore, it is assumed that $R$ is known, which is not unreasonable as many derived data products and sensors report uncertainties, and uncertainties from field data can often be inferred from sampling theory or Monte Carlo resampling / bootstrapping. In many cases new data do not exist for every state variable (e.g. we may observe leaves and wood but not soil C) and so an observation operator matrix, $H$ is also introduced, which in most cases is just a $n \times m$ indicator matrix matching the observed $Y$'s (n=2 in this example) with the vector of state variables (m=3 in this example). Indeed, one of the powerful features of data assimilation is its ability to update unobserved state variables based on the covariances between observed and unobserved variables. Putting this all together, Bayes' Theorem can then be used to update our MVN forecast prior, given a MVN Likelihood of $Y_t$ with covariance $R$, giving an analytically-tractable conjugate MVN posterior with mean $\mu_{a,t}$ and covariance $P_{a,t}$:

$$P_{a,t}^{-1} = H_t^T R_t^{-1} H_t + P_{f,t}^{-1}$$
$$\mu_{a,t} =  P_{a,t}^{-1}\left( H_t^T R^{-1} Y_{t} + P_{f,t}^{-1} \mu_{f,t} \right) $$

The subscript $a$ is used to refer to the Analysis posterior and distinguish this from the prior forecast $f$ mean and covariance for the same timestep. While the matrix notation in these equations can make these solutions look complicated, the underlying meaning is accessible. First, the precision of the analysis (i.e. 1/variance) is the sum of the observation precision and the forecast precision, meaning that the analysis is always more precise than the data or forecast alone. Second, the mean of the analysis is a weighted average between the data, $Y_t$ and the forecast $mu_{f,t}$, each weighted by their precision. This averaging can also be viewed as a 'nudge', pushing the forecast toward the new data, with the magnitude of that nudge determined by the uncertainties in both the forecast and data (i.e. highly precise data = big nudge, highly uncertain data = very small nudge).

While this fusion of forecast and new data is an incredibly powerful concept, as alluded to in the _Introduction_ there are times that the Kalman Normal-Normal "nudge" produces results that don't always make sense. Consider as a simple example a simulated data experiment where LAI growth follows a simple stochastic logistic model with growth rate $r$=0.1, equilibrium $K$=6, process error standard deviation 0.25, and observation error standard deviation 0.75. Under normal conditions, EnKF would have no trouble nudging a LAI forecast back on track every time a new observation is made, but what if a discrete disturbance event occurs knocking LAI from steady-state down to $1.0 \pm 0.5$? In this case the compromise between the forecast ($LAI=6$) and observations ($LAI=1$) produces a compromise ($LAI \approx 5$ that is a poor representation of the state of the system (Figure \@ref(fig:enkf)). Furthermore, as the true LAI slowly recovers upward, the EnKF keeps gradually nudging LAI downward until the two converge. Not only is the assimilated state wrong, but so is our estimate of the process -- a discrete disturbance and rapid recovery is behaving like gradual degradation, which will lead to incorrect inferences about related processes (litter, soil C, NPP, NEE, ET, albedo, etc.). Not also that including a disturbance probability in the process model does not solve this problem. Assuming the disturbance probability is low (e.g. a background rates of <1%), at every point in time the forecast would include a small number of disturbed ensemble members, but the overwhelming majority would be undisturbed. Therefore $\mu_{f,t}$ will be relatively unchanged and $P_{f,t}$ would see a small increase in variance. The fundamental problem is that even though the true forecast is now bimodal (a large peak at the undisturbed state and a much smaller peak at the disturbed state), the EnKF is still representing this as a unimodal Normal. Indeed, if the you increased the disturbance probability substantially (e.g. 50%), the EnKF would treat this as a unimodal forecast half way between the disturbed and undisturbed states, a place where no ensemble members are present.

```{r enkf, echo=FALSE, fig.cap="Example assimilation of a disturbance event shows that conventional assimilation approaches, such as EnKF, take time to converge to new states after disturbance (orange). By modeling the probability of disturbance (black) our Multinomial assimilation (blue) can correctly jump between states during disturbance events (e.g. P(disturbance) = 0.977 at time 10) while remaining resilient to potential false-positive events (e.g. time 12 and 15)."}
SSSD_figure <- function(){
  load("SSSD.RData")      ## simulation from SingleSite_SingleDisturbance.Rmd
  plot(data$Y,ylim = range(rbind(CI.fN,data$Y)),xlim=xlim,type='n',
     ylab="LAI",xlab="time")
  polygon(cbind(c(time, rev(time), 1), c(an0.stats[2, ], rev(an0.stats[4, ]),
    an0.stats[1, 1])), border = NA, col = '#FFBF6C')   ## EnKF interval
  ecoforecastR::ciEnvelope(1:NT,CI.fN[2,],CI.fN[4,],col=rgb(col[1],col[2],col[3],0.3*256,maxColorValue=256)) ## Multinomial interval
  lines(an0.stats[3,],col="orange",lwd=2)   ## EnKF median
  lines(CI.fN[3,],col="blue",lwd=2)         ## Multinomial median
  lines(Dbar,lwd=2)                         ## Multinomial disturbance rate
  points(data$Y,pch=18,col=2)               ## data
  legend("topright",legend=c("EnKF","Multinomial","data","P(dist)"),
       lwd=5,col=c("orange","blue","red","black")) ##"#b2df8a"
}
SSSD_figure()
```

## Derivation of the Multinomial Filter

To address this fundamental flaw in the Kalman Analysis, we need to build into the Analysis step both the possibility for ensemble forecasts to be multimodel, and the ability for the data assimilation to jump discretely between modes all-at-once rather than through gradual nudges. To begin, let's use $X$ to represent the latent (unobserved) "true" state of the system. Furthermore, we will consider a process-model that is capable of predicting $k$ different categorical outcomes (land cover classes, types of disturbance, etc.) each of which has its own prediction for the latent continuous state variables of interest, which we'll refer to as $X^L_{k,t}$. Similar to the EnKF we'll assume that the ensemble prediction for any one of these categories can be represented as a multivariate Normal based on the ensemble sample mean and covariance:

$$X^L_{k,t} \sim MVN(\vec{\mu}_{f,k,t},P_{f,k,t})$$
  
However, because we have $k$ categories we end up with a multimodel prediction. Furthermore, these modes need not be distinct; it is perfectly fine in concept for these MVNs to be overlapping. In practice they more they overlap the harder it is to identify which mode you are in, but as we will see this categorical assignment will be treated probabalistically. Indeed, to be able to represent our multimodel prediction as a single probability distribution we need to know the _forecasted_ probability of being in each category, which we'll take as known and represent using the vector $\vec{p_t}$, giving us a weighted MVN mixture as our forecast.

In practice, an individual location can only be in one of these $k$ states, which we'll model using a Multinomial distribution with a sample size of 1.
  
$$\vec{\rho_t} \sim Multinomial(1,\vec{p_t})$$ 
  
This is a generalization of the Bernoulli distribution (Binomial with n=1) and for any individual draw from the Multinomial $\vec{\rho_t}$ will be a vector of length $k$ with a 1 in the category being assigned and a 0 for all other categories. 

Within the analysis step we will retain the assumption that the observed data $Y_t$ follow a multivariate Normal distribution around the true latent $X_t$ with observation error $R_t$

$$Y_t \sim MVN(X_t,R_t)$$

We then model $X_t$ as the product of the matrix $X^L_t$ and the vector $\vec{\rho_t}$

$$X_t = X^L_t \vec{\rho_t}$$
which for any individual draw of $\vec{\rho_t}$ assigns $X_t$ that specific $X^L_{k,t}$. In practice, we will use Bayes' Theorem to integrate over the uncertainties in both our categorical assignment and continuous state variables, producing posterior Analysis probability distributions for both $X_t$ and $\vec{\rho_t}$. Unlike in the EnKF, where conjugacy allows us to calculate an analytical solution for the posterior, there is not simple analytical solution for the joint posterior distribution of $X_t$ and $\vec{\rho_t}$. In this case we will use well-developed Bayesian Markov Chain Monte Carlo (MCMC) numerical algorithms to sample from the posterior distribution. Specifically, numerical analyses in this paper were conducted using JAGS v4.2.0 [@Plummer03] accessed from R v4.0.3 [@Rcore] using the rjags package v4.10 [@rjags].

### Ensemble Adjustment

When making ensemble forecasts using the Analysis from the traditional EnKF, one needs to sample new initial conditions from the Analysis joint posterior distribution. This work fine if the initial conditions are complete (100% of the model variables represented) and when there is no covariance with any of the other uncertainties being propagated (e.g. sampling ensemble drivers or posterior parameter distributions). In practice, this assumption is frequently violated, for example an above average ensemble member might be above average not just because of its initial condition, but also because it had favorable parameters or drivers, creating a complex covariance between states, parameters, and drivers that would be destroyed by randomly resampling initial conditions. Because of this Anderson et al. [-@Anderson2001] developed an alternative know as _ensemble adjustment_ that nudges individual ensemble members toward the posterior analysis mean and covariance. Mathematically, the approach is essentially the multivariate generalization of using the prior forecast mean and variance to calculate a Z-score for every ensemble member (i.e. express as an standard Normal anomaly with mean 0 and standard deviation 1), and then apply that Z-score to the new posterior analysis mean and variance.

In our Multinomial filter it is important to generalize this concept of ensemble adjustment to both discretely move ensemble members from one category to another, and then to preserve the covariances among state variables, and with drivers and parameters, when updating the discrete categorical assignments of individual ensemble members. To reassign forecasted ensemble members to new disturbance classes we first identify the "sink" categories that increased in probability between the prior, $p_{k,t}$, and the posterior, $\bar{\rho}_{k,t}$.

$$\delta p_{k,t} = max\{0,\bar{\rho}_{k,t}-p_{k,t} \}$$

For "source" ensemble members that belong to categories that decreased in relative abundance, the excess is reassigned to new classes according to a Multinomial draw.

$$c_{A|k} = Multinomial\left( n_k, {\vec{\delta p_t}}\over{\sum \vec{\delta p_t}}  \right)$$ 

Conditional on this reassignment, the continuous states are updated according to [@Anderson2001] using the posterior statistics for that $X^L_{k,t}$. It is also worth noting that some cases of class reassignment may also require keeping track of categorical switches in Plant Functional Type (PFT), such as a transition from forest to agriculture, which implies a large jump in PFT parameters as well. One could apply the same [@Anderson2001] style adjustment to PFT parameters, but for simplicity here we just maintained alternative parameter ensembles for each PFT.

## Simulated data experiments

To test our algorithm we applied it to a series of increasingly complex simulated data experiments where we know the "true" state of the system, including the timing and category of disturbance events. The first of these is the logistic LAI model described previously in the EnKF section (single pool, single site, single disturbance. 

```{r pseudoTable,echo=FALSE}
## table summarizing simulated data experiments
exps = data.frame(exp=1:5,
                  pools= c(1,3,3,3,3),
                  disturbances = c(1,1,2,2,1),
                  PFTs = c(1,1,1,2,1),
                  sites = c(1,1,1,1,2)
                  )
knitr::kable(exps,caption="Simulated data experiments")
```


For the second experiment, we used the VSEM to extend the Multinomial assimilation model to a single site, single disturbance example with multiple pools. In this and the following simulated data experiments we used VSEM's default parameters, drawing ensemble members from Normal distributions centered on the prior mean and assuming a standard deviation that was 1/10th the default uniform prior range. The goal here was not to replicate any specific system, but to provide an amount of parameter uncertainty that was realistic but not so large as to generate trivial ensemble forecasts. Model runs were for 25 years using VSEM's default simulated PAR as the driver and with data assimilation occurring annually. Model initial conditions were generated by an initial 25 year ensemble spin-up prior to the 25 year experiment. Process error was added to the simulation annually assuming a zero-bound tobit distribution with a process error covariance matrix that was diagonal with variances of 0.1 for all pools. Ensemble members experienced disturbance stochastically with a 5% annual probability. For the disturbance itself, the post-disturbance leaf and wood carbon were drawn from a tobit with a mean of 1 for both pools, standard deviations of 0.15 and 0.25 respectively, and 25% of the biomass lost was assumed to enter the soil C pool, with the remainder removed (which could be thought of as either atmospheric emissions or biomass harvest depending on the disturbance type). 

Pseudodata for assimilation was generated by selecting the parameters and initial conditions from one ensemble member at random and running a forward simulation with an imposed disturbance event in year 10. Observation error was then randomly added to the model's predictions using a tobit distribution with the same known observation error matrix, R, as used in the assimilation, which was a diagonal covariance matrix with variances of 0.25, 10, and 1 for leaf, soil, and wood respectively. Within the assimilation the prior on the disturbance rate was 5%.

Our third experiment was similar to the second but introduced a second disturbance type (single-site, multi-disturbance, multi-pool) ....

Our forth experiment extended the single-site, multi-disturbance, multi-pool scenario to one where one disturbance type included a switch in PFT....

Our fifth and final experiment extended the analysis to a two-site scenario with spatial covariance across sites and pools, but with a disturbance occurring in one site but not at the other. To keep things simpler, this experiment reverted to a single disturbance type. This scenario was designed to test a concern we had in previous research that a disturbance occurring at one site could cause the biomass to decline at other undisturbed sites because of the cross-site covariances [CITE Viskari, Hamze]....



  + x SS Multi-Disturbance - distinct
  + x SS Multi-Disturbance w/ distinct, PFT switching
      * mention parallel PFT ensembles
      * need to protect against long-term PFT leakage; related to more general need to "look back"
  + x Simple Multi-site: spatial cov
  + Future
      * SSMD - initially overlapping but diverge
        * how to handle updating of assignment based on new information?
      * SSMD - transition probabilities change with state
        * rate of false pos/neg: affected by p.dist, R, Q, P0 -- how much do we need to simulate?
     
## Real-world examples

Extract some example time series for single pixels (e.g. AGB product) where the disturbance (2004) it is far enough in the past that there’s an interesting recovery trajectory but also where it’s not so early (so that the data assimilation has time to come into steady-state before the disturbance).

### Study region and data sources

    + Landtrendr
    http://emapr.ceoas.oregonstate.edu/pages/data/viz/index.html
    + Location:
    tile h03v04
xmin       : -2115585 
xmax       : -1965585 
ymin       : 2564805 
ymax       : 2714805 
Upper Left  (-2115585.000, 2714805.000) (123d 6'32.29"W, 44d41'17.09"N)
Lower Left  (-2115585.000, 2564805.000) (122d35' 7.51"W, 43d23'46.18"N)
Upper Right (-1965585.000, 2714805.000) (121d16'45.91"W, 45d 3'17.45"N)
Lower Right (-1965585.000, 2564805.000) (120d47'15.95"W, 43d45'18.18"N)
Center      (-2040585.000, 2639805.000) (121d56'25.42"W, 44d13'35.07"N)
> res(cover)
[1] 30 30
> crs(cover)
CRS arguments:
 +proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +datum=NAD83
+units=m +no_defs 
    + Biomass [@Kennedy2018]
      1990-2017
    + Cover
      1990-2017
    + Disturbance
      1984-2011
      - nearest neighbor resampled to same as cover

 + Applications: PNW
    + Logging
    + Fire
    + Land use transition (forest -> non?)


### Construction of disturbance frequency and transition matrix priors

* recoded to a smaller set of disturbances: none, fire, pest, clearcut, other
* Calculate frequencies of different disturbance types
* Calculate disturbance-by-landCover matrix of transition probabilities
* Use these as disturbance priors

### Calibration of VSEM

[@BayesianTools]

* Calibrate at US-Me2, 2002-2019 
* Initialize with BADM soil, leaf, and stem C
* Priors (and some fixed parameters) using BADM; default uniform for rest
* Likelihood against tower _annual_ NEE and BADM leaf and stem biomass
* MCMC using DEzs algorithm for 25000 iterations of 2 chains
* GBR, effective sample

### Comparison to real-world data

* Selected conifer forest land cover type
* Sampled ~100 points from each disturbance type for 2004
* Daymet PAR as driver
* IC
  * ensemble spin up
  * construct disturbance recovery trajectory
  * Extract rs map AGB
  * Match leaf and soil C from spin-up trajectory

* Experiments
  * run SSSDMP model for each site independently
  * run other versions varying R, Q, and the disturbance probability
  * Score: filter divergence, disturbance detection & skill
  * Regress vs: pre-disturbance biomass, size of disturbance (abs & rel), type, etc
